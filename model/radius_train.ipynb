{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T07:54:30.812645Z",
     "start_time": "2023-11-30T07:54:27.613352600Z"
    }
   },
   "id": "91a24a48c60e7953"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 22):\n",
    "       random.seed(seed)\n",
    "       np.random.seed(seed)\n",
    "seed_everything()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T07:54:36.690737600Z",
     "start_time": "2023-11-30T07:54:36.687229Z"
    }
   },
   "id": "18a26b6e0c9bac97"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "tot_actions = 6\n",
    "actions_name = ['ankle', 'fall', 'jump', 'knee', 'run', 'walk']\n",
    "min_data_len = 35"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T07:54:37.273890Z",
     "start_time": "2023-11-30T07:54:37.270383300Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "actions_csv_dir = '../dataset/radius_csv_dataset'\n",
    "dataset = []\n",
    "\n",
    "label_mapping = {'ankle': 0,\n",
    "                 'fall': 1,\n",
    "                 'jump': 2,\n",
    "                 'knee': 3,\n",
    "                 'run' : 4,\n",
    "                 'walk' : 5\n",
    "                 }\n",
    "\n",
    "def map_action_to_label(csv_name):\n",
    "       for action, label in label_mapping.items():\n",
    "              if action in csv_name.split('_')[0]:\n",
    "                     return label\n",
    "       return -1\n",
    "\n",
    "for action_csv in os.listdir(actions_csv_dir):\n",
    "       action_df = pd.read_csv(os.path.join(actions_csv_dir, action_csv))\n",
    "       label = map_action_to_label(action_csv)\n",
    "       if label != -1:\n",
    "              for idx in range(0, len(action_df), int(min_data_len / 2)):\n",
    "                     seq_df = action_df[idx: idx + min_data_len] #길이만큼 데이터 자른 것(즉 length 만큼의 프레임)\n",
    "                     if len(seq_df) == min_data_len: # 딱 length에 개수 맞춰서 끊어서 넣으려고\n",
    "                            dataset.append({'key': label, 'value': seq_df}) # key에 slide, value에는 묶음 프레임 만큼이 담기겠네\n",
    "       #최종적으로 dataset에는 행동별로 dictionary 가 만들어져 들어간다."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T07:54:37.875074600Z",
     "start_time": "2023-11-30T07:54:37.857659200Z"
    }
   },
   "id": "20776e1e9089a511"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0           1           2           3           4           5\n",
      "0   118.957524  118.957524  118.957524  118.957524  118.957524  118.957524\n",
      "1   116.456474  116.456474  116.456474  116.456474  116.456474  116.456474\n",
      "2   120.158757  120.158757  120.158757  120.158757  120.158757  120.158757\n",
      "3   117.191128  117.191128  117.191128  117.191128  117.191128  117.191128\n",
      "4   116.849897  116.849897  116.849897  116.849897  116.849897  116.849897\n",
      "5   118.158333  118.158333  118.158333  118.158333  118.158333  118.158333\n",
      "6   116.082080  116.082080  116.082080  116.082080  116.082080  116.082080\n",
      "7   115.984675  115.984675  115.984675  115.984675  115.984675  115.984675\n",
      "8   114.949566  114.949566  114.949566  114.949566  114.949566  114.949566\n",
      "9   117.917963  117.917963  117.917963  117.917963  117.917963  117.917963\n",
      "10  117.909310  117.909310  117.909310  117.909310  117.909310  117.909310\n",
      "11  116.314775  116.314775  116.314775  116.314775  116.314775  116.314775\n",
      "12  118.962365  118.962365  118.962365  118.962365  118.962365  118.962365\n",
      "13  116.502295  116.502295  116.502295  116.502295  116.502295  116.502295\n",
      "14  116.125277  116.125277  116.125277  116.125277  116.125277  116.125277\n",
      "15  117.744411  117.744411  117.744411  117.744411  117.744411  117.744411\n",
      "16  115.979039  115.979039  115.979039  115.979039  115.979039  115.979039\n",
      "17  115.659041  115.659041  115.659041  115.659041  115.659041  115.659041\n",
      "18  114.926787  114.926787  114.926787  114.926787  114.926787  114.926787\n",
      "19  118.059029  118.059029  118.059029  118.059029  118.059029  118.059029\n",
      "20  118.020843  118.020843  118.020843  118.020843  118.020843  118.020843\n",
      "21  116.530800  116.530800  116.530800  116.530800  116.530800  116.530800\n",
      "22  118.669802  118.669802  118.669802  118.669802  118.669802  118.669802\n",
      "23  116.526682  116.526682  116.526682  116.526682  116.526682  116.526682\n",
      "24  116.487673  116.487673  116.487673  116.487673  116.487673  116.487673\n",
      "25  117.257809  117.257809  117.257809  117.257809  117.257809  117.257809\n",
      "26  115.614669  115.614669  115.614669  115.614669  115.614669  115.614669\n",
      "27  115.751832  115.751832  115.751832  115.751832  115.751832  115.751832\n",
      "28  114.909463  114.909463  114.909463  114.909463  114.909463  114.909463\n",
      "29  118.312117  118.312117  118.312117  118.312117  118.312117  118.312117\n",
      "30  117.484000  117.484000  117.484000  117.484000  117.484000  117.484000\n",
      "31  119.846894  119.846894  119.846894  119.846894  119.846894  119.846894\n",
      "32  117.546475  117.546475  117.546475  117.546475  117.546475  117.546475\n",
      "33  116.354199  116.354199  116.354199  116.354199  116.354199  116.354199\n",
      "34  117.361574  117.361574  117.361574  117.361574  117.361574  117.361574\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0]['value']) # z축 까지 99 (33 * 3)차원"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T07:54:40.501989100Z",
     "start_time": "2023-11-30T07:54:40.499481500Z"
    }
   },
   "id": "d656657e15c7e45f"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "       def __init__(self, dataset): #모든 행동을 통합한 df가 들어가야함\n",
    "              self.x = []\n",
    "              self.y = []\n",
    "              for dic in dataset:\n",
    "                     self.y.append(dic['key']) #key 값에는 actions 들어감\n",
    "                     self.x.append(dic['value']) #action마다의 data 들어감\n",
    "\n",
    "       def __getitem__(self, index): #index는 행동의 index\n",
    "              data = self.x[index] # x에는 꺼내 쓸 (행동마다 45개 묶음프레임)의 데이터\n",
    "              label = self.y[index]\n",
    "              return torch.Tensor(np.array(data)), torch.tensor(np.array(int(label)))\n",
    "\n",
    "       def __len__(self):\n",
    "              return len(self.x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T07:54:46.901718800Z",
     "start_time": "2023-11-30T07:54:46.899211100Z"
    }
   },
   "id": "f6106f1224437afe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_test_val_ratio = [0.8, 0.1, 0.1]\n",
    "print(len(dataset))\n",
    "train_len = int(len(dataset) * train_test_val_ratio[0])\n",
    "val_len = int(len(dataset) * train_test_val_ratio[1])\n",
    "test_len = len(dataset) - train_len - val_len"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba330d25c6c6f524"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "CFG = {'batch_size': 8,\n",
    "       'learning_rate': 2e-2,\n",
    "       'seed': 22,\n",
    "       'epochs': 30    \n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T07:54:50.806138200Z",
     "start_time": "2023-11-30T07:54:50.793616700Z"
    }
   },
   "id": "14d2065c8d4dc0c6"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(dataset)\n",
    "train_data, valid_data, test_data = random_split(train_dataset, [train_len, val_len, test_len])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=CFG['batch_size'])\n",
    "val_loader = DataLoader(valid_data, batch_size=CFG['batch_size'])\n",
    "test_loader = DataLoader(test_data, batch_size=CFG['batch_size'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T07:54:51.372786100Z",
     "start_time": "2023-11-30T07:54:51.369784200Z"
    }
   },
   "id": "9fa7ab00e37041cc"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def shuffle_dataset(lm_list, label_list):\n",
    "       lm_list, label_list = shuffle(lm_list, label_list, random_state=22)\n",
    "       return lm_list, label_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T07:54:52.447575200Z",
     "start_time": "2023-11-30T07:54:52.256486800Z"
    }
   },
   "id": "c7be83410625fd04"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "       def __init__(self):\n",
    "              super(Model, self).__init__()\n",
    "              self.recurrent_layer = nn.LSTM(hidden_size=100, input_size=6,  bidirectional=True)\n",
    "\n",
    "              self.nonLin = nn.BatchNorm1d(35)\n",
    "              self.recurrent_layer2 = nn.LSTM(hidden_size=100, input_size=200, bidirectional=True)\n",
    "              self.nonLin2 = nn.BatchNorm1d(35)\n",
    "              self.conv = nn.Conv1d(35,36,7,1)\n",
    "              self.relu1 = nn.Softmax()\n",
    "              self.classify_layer = nn.Linear(194, 6)\n",
    "\n",
    "       def forward(self, input, h_t_1=None, c_t_1=None):\n",
    "              # the size of rnn_outputs is [batch_size, seq_len, rnn_size]\n",
    "              rnn_outputs, (hn, cn) = self.recurrent_layer(input)\n",
    "              lin1 = self.nonLin(rnn_outputs)\n",
    "\n",
    "              rnn_outputs2, (hn2, cn2) = self.recurrent_layer2(lin1)\n",
    "\n",
    "              lin2 = self.nonLin2(rnn_outputs2)\n",
    "              conv = self.conv(lin2)\n",
    "              relu = self.relu1(conv)\n",
    "\n",
    "              logits = self.classify_layer(relu[:,-1])\n",
    "              return logits\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T07:54:53.077968Z",
     "start_time": "2023-11-30T07:54:53.075460600Z"
    }
   },
   "id": "19844f8477cf9c27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "class Model(nn.Module):\n",
    "       def __init__(self):\n",
    "              super(Model, self).__init__()\n",
    "              self.lstm1 = nn.LSTM(input_size=99, hidden_size=128, num_layers=1, batch_first=True) \n",
    "              self.lstm2 = nn.LSTM(input_size=128, hidden_size=256, num_layers=1, batch_first=True)\n",
    "              self.dropout1 = nn.Dropout(0, 1)\n",
    "              self.lstm3 = nn.LSTM(input_size=256, hidden_size=128, num_layers=1, batch_first=True)\n",
    "              self.lstm4 = nn.LSTM(input_size=128, hidden_size=64, num_layers=1, batch_first=True)\n",
    "              self.dropout2 = nn.Dropout(0, 1)\n",
    "              self.lstm5 = nn.LSTM(input_size=64, hidden_size=32, num_layers=1, batch_first=True)\n",
    "              self.fc = nn.Linear(32, 6) #분류할 클래스 6가지\n",
    "\n",
    "       def forward(self, x):\n",
    "              x, _ = self.lstm1(x)\n",
    "              x, _ = self.lstm2(x)\n",
    "              x, _ = self.lstm3(x)\n",
    "              x = self.dropout1(x)\n",
    "              x, _ = self.lstm4(x)\n",
    "              x, _ = self.lstm5(x)\n",
    "              x, _ = self.lstm6(x)\n",
    "              x = self.dropout2(x)\n",
    "              x, _ = self.lstm7(x)\n",
    "              x = self.fc(x[:, -1, :])\n",
    "              return x\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "494df6883ce0d0f9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_epochs = CFG['epochs']\n",
    "best_models = []  # 폴드별로 성능 높은 모델 저장\n",
    "for i,fold in enumerate(range(4)):\n",
    "       print('===============',i+1,'fold start===============')\n",
    "       \n",
    "       model = Model()\n",
    "       optimizer = optim.Adam(model.parameters(), lr=CFG['LEARNING_RATE'] )\n",
    "       criterion = nn.CrossEntropyLoss()\n",
    "       lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                      step_size = 5,\n",
    "                                                      gamma = 0.9) # learning rate scheduler 로 학습률 주기적 감소\n",
    "       \n",
    "\n",
    "       val_acc_max = 0.9 # 정확도 0.9 이상 저장\n",
    "       val_loss_min = 0.2 #손실 0.2 이하 저장\n",
    "       \n",
    "       # Lists to store training and validation metrics for each epoch\n",
    "       train_loss_history = []\n",
    "       val_loss_history = []\n",
    "       val_acc_history = []\n",
    "       \n",
    "       for epoch in range(num_epochs):\n",
    "              train_loss_list = []\n",
    "              val_loss_list = []\n",
    "              val_acc_list = []\n",
    "       \n",
    "              # 모델 학습\n",
    "              for i, (images, targets) in enumerate(train_loader):\n",
    "                     model.train()\n",
    "       \n",
    "                     optimizer.zero_grad()\n",
    "                     outputs = model(images)\n",
    "       \n",
    "                     targets = targets.long()\n",
    "                     loss = criterion(outputs, targets)\n",
    "                     loss.backward()\n",
    "                     optimizer.step()\n",
    "       \n",
    "                     train_loss_list.append(loss.item())\n",
    "       \n",
    "                     if (i + 1) % 20 == 0:\n",
    "                            print(f'Epoch: {epoch} - Loss: {loss:.6f}')\n",
    "       \n",
    "              train_loss = np.mean(train_loss_list)\n",
    "       \n",
    "              # 모델 검증\n",
    "              for i, (images, targets) in enumerate(val_loader):\n",
    "                     model.eval()\n",
    "       \n",
    "                     with torch.no_grad():\n",
    "                            outputs = model(images)\n",
    "       \n",
    "                            targets = targets.long()\n",
    "       \n",
    "                            val_loss = criterion(outputs, targets)\n",
    "       \n",
    "                            preds = torch.argmax(outputs, dim=1)\n",
    "       \n",
    "                            batch_acc = (preds == targets).float().mean()  # boolean 값의 평균\n",
    "       \n",
    "                            val_loss_list.append(val_loss.item())\n",
    "                            val_acc_list.append(batch_acc.item())\n",
    "       \n",
    "              val_loss = np.mean(val_loss_list)\n",
    "              val_acc = np.mean(val_acc_list)\n",
    "       \n",
    "              train_loss_history.append(train_loss)\n",
    "              val_loss_history.append(val_loss)\n",
    "              val_acc_history.append(val_acc)\n",
    "       \n",
    "              print(f'Epoch: {epoch} - valid Loss: {val_loss:.6f} - valid_acc : {val_acc:.6f}')\n",
    "       \n",
    "              if val_acc_max < val_acc or val_loss_min > val_loss:\n",
    "                     val_acc_max = val_acc\n",
    "                     best_models.append(model)\n",
    "                     print('model save, model val acc : ', val_acc)\n",
    "       \n",
    "              lr_scheduler.step()\n",
    "\n",
    "print('Train finished, best_models size : ', len(best_models))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fff7d2a484ac9bd0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_directory = 'saved_models'\n",
    "\n",
    "for i, model in enumerate(best_models):\n",
    "       model_filename = f\"best_model_{i}.pth\"\n",
    "       model_path = save_directory + model_filename\n",
    "       torch.save(model.state_dict(), model_path)\n",
    "\n",
    "print(f\"Saved {len(best_models)} models to {save_directory}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61322c2e9c6b0743"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 학습, 검증 과정 손실 그래프\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_loss_history, label='Training Loss')\n",
    "plt.plot(val_loss_history, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss_for_epoch_for_4folds')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 학습, 검증 과정 정확도 그래프\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(val_acc_history, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy_for_epoch_for_4folds.png')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31c2c94df12d7ff2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 정확도 검증\n",
    "for m in best_models:\n",
    "       with torch.no_grad():\n",
    "              test_loss, test_acc = epoch(test_loader, mode='test')\n",
    "              test_acc = round(test_acc, 4)\n",
    "              test_loss = round(test_loss, 4)\n",
    "              print('Test Acc.: {}'.format(test_acc))\n",
    "              print('Test Loss: {}'.format(test_loss))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2043856aefa7c707"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_model(model, filepath):\n",
    "       model.load_state_dict(torch.load(filepath))\n",
    "       model.eval()\n",
    "       return model\n",
    "\n",
    "load_directory = save_directory \n",
    "\n",
    "num_models = 5\n",
    "test_accuracies = []\n",
    "\n",
    "for i in range(num_models):\n",
    "       model = Model()\n",
    "       model_filename = f\"best_model_{i}.pth\"\n",
    "       model_path = load_directory + model_filename\n",
    "       model = load_model(model, model_path)\n",
    "       \n",
    "       correct = 0\n",
    "       total = 0\n",
    "       with torch.no_grad():\n",
    "              for images, targets in test_loader:\n",
    "                     outputs = model(images)\n",
    "                     _, predicted = torch.max(outputs.data, 1)\n",
    "                     total += targets.size(0)\n",
    "                     correct += (predicted == targets).sum().item()\n",
    "               \n",
    "       accuracy = correct / total\n",
    "       test_accuracies.append(accuracy)\n",
    "\n",
    "       print(f\"Model {i+1} - Test Accuracy: {accuracy:.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15651ac8c33b3349"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저정된 frame의 개수: 148\n"
     ]
    }
   ],
   "source": [
    "# 실시간 영상 테스트\n",
    "interval = 1\n",
    "video_path = '../dataset/slide/slide001.mp4'\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "img_list = []\n",
    "if cap.isOpened():\n",
    "       cnt = 0\n",
    "       while True:\n",
    "              ret, img = cap.read()\n",
    "              if ret:\n",
    "                     img = cv2.resize(img, (640, 640))\n",
    "                     if cnt == interval:\n",
    "                            img_list.append(img)\n",
    "                            cnt = 0\n",
    "                     cv2.imshow(video_path, img)\n",
    "                     cv2.waitKey(1)\n",
    "                     cnt += 1\n",
    "              else:\n",
    "                     break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print('저정된 frame의 개수: {}'.format(len(img_list)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T06:57:21.466055700Z",
     "start_time": "2023-11-21T06:57:19.061334Z"
    }
   },
   "id": "307ee52ad21833ba"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시퀀스 데이터 분석 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 41/148 [00:01<00:03, 33.13it/s]C:\\Users\\USER\\anaconda3\\envs\\py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      " 33%|███▎      | 49/148 [00:01<00:03, 32.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 97/148 [00:03<00:01, 32.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 141/148 [00:04<00:00, 32.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:04<00:00, 32.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# 연속 시퀀스 분석\n",
    "from tqdm import tqdm\n",
    "model_filename = f\"best_model_1.pth\"\n",
    "model_path = load_directory + model_filename\n",
    "model = load_model(model, model_path)\n",
    "\n",
    "model.eval()\n",
    "out_img_list = []\n",
    "dataset = []\n",
    "status = 'None'\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=True, model_complexity=1, min_detection_confidence=0.7, min_tracking_confidence=0.5)\n",
    "\n",
    "print('시퀀스 데이터 분석 중...')\n",
    "xy_list_list = []\n",
    "\n",
    "for img in tqdm(img_list):\n",
    "       results = pose.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "       if not results.pose_landmarks: continue\n",
    "       xy_list = []\n",
    "       idx = 0\n",
    "       for x_and_y in results.pose_landmarks.landmark:\n",
    "              xy_list.append(x_and_y.x)\n",
    "              xy_list.append(x_and_y.y)\n",
    "              x, y = int(x_and_y.x * 640), int(x_and_y.y * 640)\n",
    "       idx += 1\n",
    "       xy_list_list.append(xy_list)\n",
    "\n",
    "       length = 45\n",
    "       if len(xy_list_list) == length:\n",
    "              dataset = []\n",
    "              dataset.append({'key': 0, 'value': xy_list_list})\n",
    "              dataset = MyDataset(dataset)\n",
    "              dataset = DataLoader(dataset)\n",
    "              xy_list_list = []\n",
    "              for data, label in dataset:\n",
    "                     with torch.no_grad():\n",
    "                            result = model(data)\n",
    "                            _, out = torch.max(result, 1)\n",
    "                            print(out.item())\n",
    "       cv2.putText(img, status, (0, 50), cv2.FONT_HERSHEY_COMPLEX, 1.5, (0, 0, 255), 2)\n",
    "       out_img_list.append(img)      \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T07:22:23.774213700Z",
     "start_time": "2023-11-21T07:22:19.191795400Z"
    }
   },
   "id": "3e8d1b58921162dd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
